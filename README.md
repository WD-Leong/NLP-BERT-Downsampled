# NLP-BERT-Downsampled
This repository contains a simple implementation to extend the [BERT model](https://arxiv.org/abs/1810.04805) to process longer sequences by applying average pooling of the sequence before applying the self-attention mechanism. It has two modes - (i) the model trains without any pre-training by using only the `CLS` output embeddings to train the classifier, and (ii) the model applies pre-training to generate the word embeddings prior to fine-tuning. The codes are still being refined.
