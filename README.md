# NLP-BERT-Downsampled
This repository contains a simple implementation to extend the [BERT model](https://arxiv.org/abs/1810.04805) to process longer sequences by applying average pooling of the sequence before applying the self-attention mechanism. Please note that this work is still in progress.
